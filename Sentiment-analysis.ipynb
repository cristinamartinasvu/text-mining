{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Vader on a set of IMDB reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note:* please run the following cell with all the needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import json\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from sklearn.datasets import load_files\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load('en_core_web_sm') # 'en_core_web_sm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get imdbreviews folder path\n",
    "cwd = pathlib.Path.cwd()\n",
    "imdb_reviews_folder = cwd.joinpath('IMDB-reviews-Stanford')\n",
    "\n",
    "#load files from the path\n",
    "imdb_reviews = load_files(str(imdb_reviews_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arrange data in a dataframe for easier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero Day leads you to think, even re-think why...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Words can't describe how bad this movie is. I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everyone plays their part pretty well in this ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are a lot of highly talented filmmakers/...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've just had the evidence that confirmed my s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>089: Footlight Parade (1933) - released 9/30/1...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Deeply humorous yet honest comedy about a bunc...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1st watched 2/28/2006 - 4 out of 10(Dir-Sydney...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>I watch lots of scary movies (or at least they...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>Absolutely the worst film yet by Burton, who s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label\n",
       "0      Zero Day leads you to think, even re-think why...   neutral\n",
       "1      Words can't describe how bad this movie is. I ...  negative\n",
       "2      Everyone plays their part pretty well in this ...   neutral\n",
       "3      There are a lot of highly talented filmmakers/...  negative\n",
       "4      I've just had the evidence that confirmed my s...  negative\n",
       "...                                                  ...       ...\n",
       "24995  089: Footlight Parade (1933) - released 9/30/1...   neutral\n",
       "24996  Deeply humorous yet honest comedy about a bunc...   neutral\n",
       "24997  1st watched 2/28/2006 - 4 out of 10(Dir-Sydney...  negative\n",
       "24998  I watch lots of scary movies (or at least they...  negative\n",
       "24999  Absolutely the worst film yet by Burton, who s...  negative\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all texts and labels into a uniform dataframe from the folder\n",
    "\n",
    "correct_labels = pd.DataFrame()\n",
    "texts = [doc.decode('utf-8') for doc in imdb_reviews.data]\n",
    "labels = imdb_reviews.target\n",
    "label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "string_labels = [label_mapping[label] for label in labels]\n",
    "\n",
    "correct_labels['text'] = texts\n",
    "correct_labels['label'] = string_labels\n",
    "\n",
    "correct_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define function so we can run vader in different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define function to run vader in different settings\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run VADER (as it is) on the set of imdb reveiews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.54      0.64     12500\n",
      "    negative       0.45      0.00      0.00     12500\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.27     25000\n",
      "   macro avg       0.41      0.18      0.21     25000\n",
      "weighted avg       0.62      0.27      0.32     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "just_vader = pd.DataFrame()\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sent in imdb_reviews.data:\n",
    "    sent = sent.decode('utf-8')\n",
    "    vader_output = vader_model.polarity_scores(sent)\n",
    "    sentences.append(sent)\n",
    "    labels.append(vader_output_to_label(vader_output))\n",
    "\n",
    "just_vader['text'] = sentences\n",
    "just_vader['label'] = labels\n",
    "\n",
    "report = classification_report(correct_labels['label'], just_vader['label'], target_names=['positive', 'negative', 'neutral'])\n",
    "print(report)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run VADER on the set of imdb reviews after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.55      0.64     12500\n",
      "    negative       0.55      0.00      0.00     12500\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.27     25000\n",
      "   macro avg       0.44      0.18      0.21     25000\n",
      "weighted avg       0.66      0.27      0.32     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "lemmatized = pd.DataFrame()\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sent in imdb_reviews.data:\n",
    "    sent = sent.decode('utf-8')\n",
    "    vader_output = run_vader(sent, lemmatize=True)\n",
    "    sentences.append(sent)\n",
    "    labels.append(vader_output_to_label(vader_output))\n",
    "\n",
    "lemmatized['text'] = sentences\n",
    "lemmatized['label'] = labels\n",
    "\n",
    "report = classification_report(correct_labels['label'], lemmatized['label'], target_names=['positive', 'negative', 'neutral'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run VADER on the set of imdb reviews with only adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.50      0.62     12500\n",
      "    negative       0.49      0.01      0.03     12500\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.26     25000\n",
      "   macro avg       0.44      0.17      0.22     25000\n",
      "weighted avg       0.65      0.26      0.32     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "adjectives = pd.DataFrame()\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sent in imdb_reviews.data:\n",
    "    sent = sent.decode('utf-8')\n",
    "    vader_output = run_vader(sent, parts_of_speech_to_consider={'ADJ'})\n",
    "    sentences.append(sent)\n",
    "    labels.append(vader_output_to_label(vader_output))\n",
    "\n",
    "adjectives['text'] = sentences\n",
    "adjectives['label'] = labels\n",
    "\n",
    "report = classification_report(correct_labels['label'], adjectives['label'], target_names=['positive', 'negative', 'neutral'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run VADER on the set of imdb reviews with only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.60      0.49      0.54     12500\n",
      "    negative       0.51      0.08      0.14     12500\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.29     25000\n",
      "   macro avg       0.37      0.19      0.23     25000\n",
      "weighted avg       0.55      0.29      0.34     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nouns = pd.DataFrame()\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sent in imdb_reviews.data:\n",
    "    sent = sent.decode('utf-8')\n",
    "    vader_output = run_vader(sent, parts_of_speech_to_consider={'NOUN'})\n",
    "    sentences.append(sent)\n",
    "    labels.append(vader_output_to_label(vader_output))\n",
    "\n",
    "nouns['text'] = sentences\n",
    "nouns['label'] = labels\n",
    "\n",
    "report = classification_report(correct_labels['label'], nouns['label'], target_names=['positive', 'negative', 'neutral'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run VADER on the set of imdb reviews with only verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/helia/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.48      0.54     12500\n",
      "    negative       0.51      0.08      0.14     12500\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.28     25000\n",
      "   macro avg       0.38      0.18      0.23     25000\n",
      "weighted avg       0.57      0.28      0.34     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = pd.DataFrame()\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sent in imdb_reviews.data:\n",
    "    sent = sent.decode('utf-8')\n",
    "    vader_output = run_vader(sent, parts_of_speech_to_consider={'VERB'})\n",
    "    sentences.append(sent)\n",
    "    labels.append(vader_output_to_label(vader_output))\n",
    "\n",
    "verbs['text'] = sentences\n",
    "verbs['label'] = labels\n",
    "\n",
    "report = classification_report(correct_labels['label'], verbs['label'], target_names=['positive', 'negative', 'neutral'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
